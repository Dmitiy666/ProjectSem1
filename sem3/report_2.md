# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 67 итерации (от 16 до 83). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/V.PNG"/>

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 40 итерации (от 16 до 56). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/Q.PNG"/>

**Вывод:** У алгоритма V learning сходиомсть достигается за 67 иттераций,

у алгоритма Q learning сходимость достигается за 40 иттераций, а средняя награда 0,9

в отличии от алгоритма ценностей, который быстрее достигает сходимости,

но с меньшей наградой. То есть основным параметром является скорость обучения



## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 114 итерации (от 26 до 140). 

Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/3S.PNG"/>

Для алгоритма `V learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 13 итерации (от 14 до 27). 

Графики зависимости reward от количества итераций приведены ниже. 

**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к уменьшению количества иттераций Это связано с тем, что 

гамма отвечает за то на сколько агент будет жадным

Уменьшая параметр гамма агент делается более жадным, поэтому требуется больше времени

<img src="image/3SS.PNG"/>

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)

Для алгоритма Q learning на поле (8х8) при gamma=0.9 сходимость (mean reward > 0.9) достигается в

среднем за 120 итераций (от 22 до 142). Графики зависимости reward от количества итераций приведены ниже.

<img src="image/s4q.PNG"/>

Для алгоритма V learning на поле (8х8) при gamma=0.9 сходимость (mean reward > 0.85) достигается в

среднем за 870 итераций (от 580 до 1 450). Графики зависимости reward от количества итераций приведены ниже.

<img src="image/s4v.PNG"/>

**Вывод:**
: На большем поле агенту необходимо больше иттераций чтобы достигнуть сходимости