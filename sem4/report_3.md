# Отчет 3. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение c изменением параметра альфы

Для алгоритма `V learning` на поле (4х4) при `alpha=0.2` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/l1.png"/>

Для алгоритма `Q learning` на поле (4х4) при `alpha=0.5` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 16 до 56). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/l2.png"/>

Для алгоритма `Q learning` на поле (4х4) при `alpha=1` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 16 до 56). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/l3.PNG"/>

**Вывод:** Алгоритм обучения ценности состояний более эффективен чем обучение ценности действий. Это связано с тем, что ... 
