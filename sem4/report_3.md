# Отчет 3. Отчет 3. Аппроксимация функции ценности действия с помощью искуственной нейронной сети DQN(s,a).

## 1. Сравнение c изменением параметра альфы1. Влияние гиперпараметра альфа на среднее количество шагов обучения (2 балла)

Для алгоритма `Q learning` на поле (4х4) при `alpha=0.2` сходимость (mean reward > 0.85) достигается в среднем за 4721 итерации (от 1020 до 10462). 

Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/l1.png"/>

Для алгоритма `Q learning` на поле (4х4) при `alpha=0.5` сходимость (mean reward > 0.85) достигается в среднем за 4300 итерации (от 2200 до 6500). 

Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/l2.png"/>

Для алгоритма `Q learning` на поле (4х4) при `alpha=1` сходимость (mean reward > 0.85) достигается в среднем за 10000 итерации (от 2100 до 17900). 

Графики зависимости reward от количества итераций приведены ниже. 

<img src="image/l3.PNG"/>

**Вывод:** Алгоритм обучения ценности состояний более эффективен чем обучение ценности действий. Это связано с тем, что 

гиперпараметр alpha сильно влияет на скорость обучения модели. Не трудно заметить, что если alpha стремится к нулю, то

начинаются проблемы в обучении модели: обучение сильно замедляется или становится невозможным вследствии локальных минимумов.


## №2 Изучите алгоритм глубокого обучения (Deep Q learning) в среде Pong: Chapter06/02_dqn_pong.py Обучите сеть с гиперпараметрами по умолчанию и запишите видео Chapter06/03_dqn_play.py (2 балла).

Мы обучили алгоритм и получили следующий результат: Total reward: -7.00. 

На видео видно, как модель практически одну минуту справляется с игрой. 

Данный результат не является лучшим из возможных, но наш агент хорошо справляется 

с задачей.

## №3 Влияние гиперпараметров (Deep Q learning в среде Pong) на среднее количество шагов обучения. (2 балла)

Будет изменен гипермараметр batch_size с 32 до 16 объектов и посмотрим как поведет себя агент в таких условиях. 